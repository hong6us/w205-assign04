{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r56c77c4db3250be6_0000016646714831_1 ... (0s) Current status: DONE   \n",
      "+--------+\n",
      "|  f0_   |\n",
      "+--------+\n",
      "| 983648 |\n",
      "+--------+\n"
     ]
    }
   ],
   "source": [
    "!bq query --use_legacy_sql=FALSE 'SELECT count(*) FROM `bigquery-public-data.san_francisco.bikeshare_trips`'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of trips in the morning and afternoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r30831e93cb6d0702_0000016640a393af_1 ... (1s) Current status: DONE   \n",
      "+----------+\n",
      "| AM_Count |\n",
      "+----------+\n",
      "|   412339 |\n",
      "+----------+\n"
     ]
    }
   ],
   "source": [
    "!bq query --use_legacy_sql=false 'SELECT count(trip_id) as AM_Count FROM `bigquery-public-data.san_francisco.bikeshare_trips` WHERE EXTRACT(HOUR FROM start_date)<12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r722e44b587ecfa00_000001663750c50f_1 ... (1s) Current status: DONE   \n",
      "+----------+\n",
      "| PM_Count |\n",
      "+----------+\n",
      "|   571309 |\n",
      "+----------+\n"
     ]
    }
   ],
   "source": [
    "!bq query --use_legacy_sql=false 'SELECT count(trip_id) as PM_Count FROM `bigquery-public-data.san_francisco.bikeshare_trips` WHERE EXTRACT(HOUR FROM start_date)>=12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of stations by landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r2b478dfe2efabc84_000001663c66cc29_1 ... (0s) Current status: DONE   \n",
      "+---------------+---------------+\n",
      "|   landmark    | Station_Count |\n",
      "+---------------+---------------+\n",
      "| San Jose      |            18 |\n",
      "| Palo Alto     |             5 |\n",
      "| Mountain View |             7 |\n",
      "| Redwood City  |             7 |\n",
      "| San Francisco |            37 |\n",
      "+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "!bq query --use_legacy_sql=false 'SELECT landmark, count(station_id) as Station_Count FROM `bigquery-public-data.san_francisco.bikeshare_stations`GROUP BY landmark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the most popular landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r6ec04fbf3d195fb2_0000016640a6551f_1 ... (0s) Current status: DONE   \n",
      "+--------+---------------+\n",
      "| Count  |   landmark    |\n",
      "+--------+---------------+\n",
      "|   4996 | Redwood City  |\n",
      "|  52861 | San Jose      |\n",
      "|  24679 | Mountain View |\n",
      "| 891223 | San Francisco |\n",
      "|   9889 | Palo Alto     |\n",
      "+--------+---------------+\n"
     ]
    }
   ],
   "source": [
    "!bq query --use_legacy_sql=false 'SELECT count(start_station_id) AS Count, landmark FROM `bigquery-public-data.san_francisco.bikeshare_trips` AS tb1 LEFT JOIN `bigquery-public-data.san_francisco.bikeshare_stations` AS tb2 ON tb1.start_station_id = tb2.station_id GROUP BY landmark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Landmark = !bq query --use_legacy_sql=false 'SELECT count(start_station_id) AS Station_Count, landmark FROM `bigquery-public-data.san_francisco.bikeshare_trips` AS tb1 LEFT JOIN `bigquery-public-data.san_francisco.bikeshare_stations` AS tb2 ON tb1.start_station_id = tb2.station_id GROUP BY landmark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Waiting on bqjob_r6b390e34fd93ec44_000001663c79d1b6_1 ... (0s) Current status: RUNNING',\n",
       " '                                                                                      ',\n",
       " 'Waiting on bqjob_r6b390e34fd93ec44_000001663c79d1b6_1 ... (1s) Current status: RUNNING',\n",
       " '                                                                                      ',\n",
       " 'Waiting on bqjob_r6b390e34fd93ec44_000001663c79d1b6_1 ... (1s) Current status: DONE   ',\n",
       " '+---------------+---------------+',\n",
       " '| Station_Count |   landmark    |',\n",
       " '+---------------+---------------+',\n",
       " '|          4996 | Redwood City  |',\n",
       " '|         52861 | San Jose      |',\n",
       " '|         24679 | Mountain View |',\n",
       " '|        891223 | San Francisco |',\n",
       " '|          9889 | Palo Alto     |',\n",
       " '+---------------+---------------+']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waiting on bqjob_r32455652a555401b_0000016640a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+---------------+---------------+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>| Station_Count |   landmark    |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+---------------+---------------+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>|          4996 | Redwood City  |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>|         52861 | San Jose      |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>|         24679 | Mountain View |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>|        891223 | San Francisco |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>|          9889 | Palo Alto     |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>+---------------+---------------+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0                                                    \n",
       "1   Waiting on bqjob_r32455652a555401b_0000016640a...\n",
       "2                   +---------------+---------------+\n",
       "3                   | Station_Count |   landmark    |\n",
       "4                   +---------------+---------------+\n",
       "5                   |          4996 | Redwood City  |\n",
       "6                   |         52861 | San Jose      |\n",
       "7                   |         24679 | Mountain View |\n",
       "8                   |        891223 | San Francisco |\n",
       "9                   |          9889 | Palo Alto     |\n",
       "10                  +---------------+---------------+"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Waiting on bqjob_r32455652a555401b_0000016640a66b5e_1 ... (0s) Current status: DONE   ',\n",
       " '+---------------+---------------+',\n",
       " '| Station_Count |   landmark    |',\n",
       " '+---------------+---------------+',\n",
       " '|          4996 | Redwood City  |',\n",
       " '|         52861 | San Jose      |',\n",
       " '|         24679 | Mountain View |',\n",
       " '|        891223 | San Francisco |',\n",
       " '|          9889 | Palo Alto     |',\n",
       " '+---------------+---------------+']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 4 v2.ipynb  Assignment 4.ipynb  annot_fpid.json  lp_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python script for interacting with BigQuery.\r\n",
      "\r\n",
      "\r\n",
      "USAGE: bq [--global_flags] <command> [--command_flags] [args]\r\n",
      "\r\n",
      "\r\n",
      "Any of the following commands:\r\n",
      "  cancel, cp, extract, head, help, init, insert, load, ls, mk, mkdef, partition,\r\n",
      "  query, rm, shell, show, update, version, wait\r\n",
      "\r\n",
      "\r\n",
      "cancel     Request a cancel and waits for the job to be cancelled.\r\n",
      "\r\n",
      "           Requests a cancel and then either: a) waits until the job is done if\r\n",
      "           the sync flag is set [default], or b) returns immediately if the sync\r\n",
      "           flag is not set. Not all job types support a cancel, an error is\r\n",
      "           returned if it cannot be cancelled. Even for jobs that support a\r\n",
      "           cancel, success is not guaranteed, the job may have completed by the\r\n",
      "           time the cancel request is noticed, or the job may be in a stage\r\n",
      "           where it cannot be cancelled.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq cancel job_id # Requests a cancel and waits until the job is done.\r\n",
      "           bq --nosync cancel job_id # Requests a cancel and returns\r\n",
      "           immediately.\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           job_id: Job ID to cancel.\r\n",
      "\r\n",
      "cp         Copies one table to another.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq cp dataset.old_table dataset2.new_table\r\n",
      "           bq cp --destination_kms_key=kms_key dataset.old_table\r\n",
      "           dataset2.new_table\r\n",
      "\r\n",
      "extract    Perform an extract operation of source_table into destination_uris.\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           extract <source_table> <destination_uris>\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq extract ds.summary gs://mybucket/summary.csv\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           source_table: Source table to extract.\r\n",
      "           destination_uris: One or more Google Cloud Storage URIs, separated by\r\n",
      "           commas.\r\n",
      "\r\n",
      "head       Displays rows in a table.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq head dataset.table\r\n",
      "           bq head -j job\r\n",
      "           bq head -n 10 dataset.table\r\n",
      "           bq head -s 5 -n 10 dataset.table\r\n",
      "\r\n",
      "help       Help for all or selected command:\r\n",
      "               bq help [<command>]\r\n",
      "\r\n",
      "           To retrieve help with global flags:\r\n",
      "               bq --help\r\n",
      "\r\n",
      "           To retrieve help with flags only from the main module:\r\n",
      "               bq --helpshort [<command>]\r\n",
      "\r\n",
      "init       Authenticate and create a default .bigqueryrc file.\r\n",
      "\r\n",
      "insert     Inserts rows in a table.\r\n",
      "\r\n",
      "           Inserts the records formatted as newline delimited JSON from file\r\n",
      "           into the specified table. If file is not specified, reads from stdin.\r\n",
      "           If there were any insert errors it prints the errors to stdout.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq insert dataset.table /tmp/mydata.json\r\n",
      "           echo '{\"a\":1, \"b\":2}' | bq insert dataset.table\r\n",
      "\r\n",
      "           Template table examples: Insert to dataset.template_suffix table\r\n",
      "           using dataset.template table as its template.\r\n",
      "           bq insert -x=_suffix dataset.table /tmp/mydata.json\r\n",
      "\r\n",
      "load       Perform a load operation of source into destination_table.\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           load <destination_table> <source> [<schema>]\r\n",
      "\r\n",
      "           The <destination_table> is the fully-qualified table name of table to\r\n",
      "           create, or append to if the table already exists.\r\n",
      "\r\n",
      "           The <source> argument can be a path to a single local file, or a\r\n",
      "           comma-separated list of URIs.\r\n",
      "\r\n",
      "           The <schema> argument should be either the name of a JSON file or a\r\n",
      "           text schema. This schema should be omitted if the table already has\r\n",
      "           one.\r\n",
      "\r\n",
      "           In the case that the schema is provided in text form, it should be a\r\n",
      "           comma-separated list of entries of the form name[:type], where type\r\n",
      "           will default to string if not specified.\r\n",
      "\r\n",
      "           In the case that <schema> is a filename, it should contain a single\r\n",
      "           array object, each entry of which should be an object with properties\r\n",
      "           'name', 'type', and (optionally) 'mode'. See the online documentation\r\n",
      "           for more detail:\r\n",
      "           https://developers.google.com/bigquery/preparing-data-for-bigquery\r\n",
      "\r\n",
      "           Note: the case of a single-entry schema with no type specified is\r\n",
      "           ambiguous; one can use name:string to force interpretation as a\r\n",
      "           text schema.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq load ds.new_tbl ./info.csv ./info_schema.json\r\n",
      "           bq load ds.new_tbl gs://mybucket/info.csv ./info_schema.json\r\n",
      "           bq load ds.small gs://mybucket/small.csv name:integer,value:string\r\n",
      "           bq load ds.small gs://mybucket/small.csv field1,field2,field3\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           destination_table: Destination table name.\r\n",
      "           source: Name of local file to import, or a comma-separated list of\r\n",
      "           URI paths to data to import.\r\n",
      "           schema: Either a text schema or JSON file, as above.\r\n",
      "\r\n",
      "ls         List the objects contained in the named collection.\r\n",
      "\r\n",
      "           List the objects in the named project or dataset. A trailing : or .\r\n",
      "           can be used to signify a project or dataset.\r\n",
      "           * With -j, show the jobs in the named project.\r\n",
      "           * With -p, show all projects.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq ls\r\n",
      "           bq ls -j proj\r\n",
      "           bq ls -p -n 1000\r\n",
      "           bq ls mydataset\r\n",
      "           bq ls -a\r\n",
      "           bq ls --filter labels.color:red\r\n",
      "           bq ls --filter 'labels.color:red labels.size:*'\r\n",
      "           bq ls --transfer_config --transfer_location='us'\r\n",
      "           --filter='dataSourceIds:play,adwords'\r\n",
      "           bq ls --transfer_run --filter='states:SUCCESSED,PENDING'\r\n",
      "           --run_attempt='LATEST' projects/p/locations/l/transferConfigs/c\r\n",
      "           bq ls --transfer_log --message_type='messageTypes:INFO,ERROR'\r\n",
      "           projects/p/locations/l/transferConfigs/c/runs/r\r\n",
      "\r\n",
      "mk         Create a dataset, table, view, or transfer configuration with this\r\n",
      "           name.\r\n",
      "\r\n",
      "           See 'bq help load' for more information on specifying the schema.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq mk new_dataset\r\n",
      "           bq mk new_dataset.new_table\r\n",
      "           bq --dataset_id=new_dataset mk table\r\n",
      "           bq mk -t new_dataset.newtable name:integer,value:string\r\n",
      "           bq mk --view='select 1 as num' new_dataset.newview\r\n",
      "           (--view_udf_resource=path/to/file.js)\r\n",
      "           bq mk -d --data_location=EU new_dataset\r\n",
      "           bq mk --transfer_config --target_dataset=dataset --display_name=name\r\n",
      "           -p='{\"param\":\"value\"}' --data_source=source\r\n",
      "           bq mk --transfer_run --start_time={start_time} --end_time={end_time}\r\n",
      "           projects/p/locations/l/transferConfigs/c\r\n",
      "\r\n",
      "mkdef      Emits a definition in JSON for a GCS backed table.\r\n",
      "\r\n",
      "           The output of this command can be redirected to a file and used for\r\n",
      "           the external_table_definition flag with the \"bq query\" and \"bq mk\"\r\n",
      "           commands. It produces a definition with the most commonly used values\r\n",
      "           for options. You can modify the output to override option values.\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           mkdef <source_uris> [<schema>]\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq mkdef 'gs://bucket/file.csv' field1:integer,field2:string\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           source_uris: a comma-separated list of uris.\r\n",
      "           schema: The <schema> argument should be either the name of a JSON\r\n",
      "           file or\r\n",
      "           a text schema.\r\n",
      "\r\n",
      "           In the case that the schema is provided in text form, it should be a\r\n",
      "           comma-separated list of entries of the form name[:type], where type\r\n",
      "           will\r\n",
      "           default to string if not specified.\r\n",
      "\r\n",
      "           In the case that <schema> is a filename, it should contain a\r\n",
      "           single array object, each entry of which should be an object with\r\n",
      "           properties 'name', 'type', and (optionally) 'mode'. See the online\r\n",
      "           documentation for more detail:\r\n",
      "           https://developers.google.com/bigquery/preparing-data-for-bigquery\r\n",
      "\r\n",
      "           Note: the case of a single-entry schema with no type specified is\r\n",
      "           ambiguous; one can use name:string to force interpretation as a\r\n",
      "           text schema.\r\n",
      "\r\n",
      "partition  Copies source tables into partitioned tables.\r\n",
      "\r\n",
      "           Usage: bq partition <source_table_prefix>\r\n",
      "           <destination_partitioned_table>\r\n",
      "\r\n",
      "           Copies tables of the format <source_table_prefix><YYYYmmdd> to a\r\n",
      "           destination partitioned table, with the date suffix of the source\r\n",
      "           tables becoming the partition date of the destination table\r\n",
      "           partitions.\r\n",
      "\r\n",
      "           If the destination table does not exist, one will be created with a\r\n",
      "           schema and that matches the last table that matches the supplied\r\n",
      "           prefix.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq partition dataset1.sharded_ dataset2.partitioned_table\r\n",
      "\r\n",
      "query      Execute a query.\r\n",
      "\r\n",
      "           Query should be specifed on command line, or passed on stdin.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq query 'select count(*) from publicdata:samples.shakespeare'\r\n",
      "           echo 'select count(*) from publicdata:samples.shakespeare' | bq query\r\n",
      "\r\n",
      "           Usage:\r\n",
      "           query [<sql_query>]\r\n",
      "\r\n",
      "rm         Delete the dataset, table, or transfer config described by\r\n",
      "           identifier.\r\n",
      "\r\n",
      "           Always requires an identifier, unlike the show and ls commands. By\r\n",
      "           default, also requires confirmation before deleting. Supports the -d\r\n",
      "           -t flags to signify that the identifier is a dataset or table.\r\n",
      "           * With -f, don't ask for confirmation before deleting.\r\n",
      "           * With -r, remove all tables in the named dataset.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq rm ds.table\r\n",
      "           bq rm -r -f old_dataset\r\n",
      "           bq rm --transfer_config=projects/p/locations/l/transferConfigs/c\r\n",
      "\r\n",
      "shell      Start an interactive bq session.\r\n",
      "\r\n",
      "show       Show all information about an object.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq show -j <job_id>\r\n",
      "           bq show dataset\r\n",
      "           bq show [--schema] dataset.table\r\n",
      "           bq show [--view] dataset.view\r\n",
      "           bq show --transfer_config projects/p/locations/l/transferConfigs/c\r\n",
      "           bq show --transfer_run\r\n",
      "           projects/p/locations/l/transferConfigs/c/runs/r\r\n",
      "           bq show --encryption_service_account\r\n",
      "\r\n",
      "update     Updates a dataset, table, view or transfer configuration with this\r\n",
      "           name.\r\n",
      "\r\n",
      "           See 'bq help load' for more information on specifying the schema.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq update --description \"Dataset description\" existing_dataset\r\n",
      "           bq update --description \"My table\" existing_dataset.existing_table\r\n",
      "           bq update -t existing_dataset.existing_table\r\n",
      "           name:integer,value:string\r\n",
      "           bq update --destination_kms_key\r\n",
      "           projects/p/locations/l/keyRings/r/cryptoKeys/k\r\n",
      "           existing_dataset.existing_table\r\n",
      "           bq update --view='select 1 as num' existing_dataset.existing_view\r\n",
      "           (--view_udf_resource=path/to/file.js)\r\n",
      "           bq update --transfer_config --display_name=name\r\n",
      "           -p='{\"param\":\"value\"}'\r\n",
      "           projects/p/locations/l/transferConfigs/c\r\n",
      "           bq update --transfer_config --target_dataset=dataset\r\n",
      "           --refresh_window_days=5 --update_credentials\r\n",
      "           projects/p/locations/l/transferConfigs/c\r\n",
      "\r\n",
      "version    Return the version of bq.\r\n",
      "\r\n",
      "wait       Wait some number of seconds for a job to finish.\r\n",
      "\r\n",
      "           Poll job_id until either (1) the job is DONE or (2) the specified\r\n",
      "           number of seconds have elapsed. Waits forever if unspecified. If no\r\n",
      "           job_id is specified, and there is only one running job, we poll that\r\n",
      "           job.\r\n",
      "\r\n",
      "           Examples:\r\n",
      "           bq wait # Waits forever for the currently running job.\r\n",
      "           bq wait job_id # Waits forever\r\n",
      "           bq wait job_id 100 # Waits 100 seconds\r\n",
      "           bq wait job_id 0 # Polls if a job is done, then returns immediately.\r\n",
      "           # These may exit with a non-zero status code to indicate \"failure\":\r\n",
      "           bq wait --fail_on_error job_id # Succeeds if job succeeds.\r\n",
      "           bq wait --fail_on_error job_id 100 # Succeeds if job succeeds in 100\r\n",
      "           sec.\r\n",
      "\r\n",
      "           Arguments:\r\n",
      "           job_id: Job ID to wait on.\r\n",
      "           secs: Number of seconds to wait (must be >= 0).\r\n",
      "\r\n",
      "\r\n",
      "Run 'bq --help' to get help for global flags.\r\n",
      "Run 'bq help <command>' to get help for <command>.\r\n"
     ]
    }
   ],
   "source": [
    "!bq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can define \"commuter trips\" as the daily routine hours. Based on query assignment 3, these are the top 5 popular trips. 8am, 5pm, 9am, 4pm, 6pm.\n",
    "#### We can also define as 5 types of \"commuter trips\", this will give a broader view: Daily commuter trips; Popular trips based on landmark; Trips with most used stations; Subscriber trips and Customer trips; Most trips by zipcode.  This is based on the past 2 weeks queries and this week's query.  \n",
    "#### The most popular hours are the normal commuter hours described above.  The top two landmarks are San Francisco and San Jose, this also aligned with the number of stations each location has. The normal commuter hours also are the subscriber hours.  But the longest trip is a customer trip, which might mean the person did a continue long trip or he/she might have kept the bike and didn't return.\n",
    "#### Another intesting finding is the number of station ratio between San Jose and San Francisco is about 1 to 2, but the number of trips ratio is about 1 to 16.  San Francisco has much more trips and most stations, but there might be the needs to add more stations in both places to bring more bikers.\n",
    "#### The data have some inconsistancy such the \"Subscriber = annual or 30-day member; Customer = 24-hour or 3-day member\", but the site offers single trip, daily, 30 day member and yearly member."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the query findings for the 3 weeks, we can recommand these offers:\n",
    "\n",
    "#### 1. Offer deals to existing subscribers and customers to refer friends or family, who will recieve discount.   \n",
    "#### 2. Since most of the commuter trips are during week day, offer weekend package deals to have family/friends trips in the most popular landmark.  Based on the finding, most these trips are in San Francisco and San Jose, the daily commuters can bring family/friends to do group bike trip on the weekends.\n",
    "#### 3. Deals can also be offered based on zip_code, it shows the demographic of bike commuters, if they bring others to subscribe, they will recieve deals in other type of business, which can be partnered with.\n",
    "#### 4. There are other types of deals can be offered for example to combine the membership with Gym memebership, but that is beyond the data provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
